{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25979e38-02b7-47a4-a1df-5e660c16fb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbfb71-6dbf-4071-9f68-4d17e85c219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Naive Bayes\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# read dataset\n",
    "data = pd.read_csv(\"training.300000.processed.noemoticon.csv\")\n",
    "\n",
    "# Download the stopwords if necessary\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Define function to remove non-alphabet characters and convert to lowercase\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Define function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "# Apply preprocessing steps\n",
    "data['text'] = data['text'].apply(remove_html_tags)\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "data['text'] = data['text'].str.split()\n",
    "data['text'] = data['text'].apply(remove_stopwords)\n",
    "\n",
    "# Print preprocessed data\n",
    "print(data['text'])\n",
    "\n",
    "# Rejoin preprocessed text data into a single string\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Divide Features and Labels\n",
    "X = data[\"text\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# Convert to vector\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "# Divide training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a Naive Bayes Classifier\n",
    "classifier_NaiveBayes = MultinomialNB()\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid_NaiveBayes = {\n",
    "    'alpha': [0.1, 0.5, 1, 2, 5, 10],\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter search and cross-validation\n",
    "grid_search = GridSearchCV(classifier_NaiveBayes, param_grid_NaiveBayes, cv=5)\n",
    "\n",
    "# Get program start time\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get program end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate program execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Output the best parameter combination and run time\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"execution time:\", execution_time, \"s\")\n",
    "\n",
    "# predict\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, confusion matrix\n",
    "accuracy_nb = accuracy_score(y_test, y_pred)\n",
    "precision_nb = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_nb = recall_score(y_test, y_pred, average='weighted')\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# output result\n",
    "print(\"Accuracy: \", accuracy_nb)\n",
    "print(\"Precision: \", precision_nb)\n",
    "print(\"Recall: \", recall_nb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "####################KNN\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import time\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# read dataset\n",
    "data = pd.read_csv(\"training.300000.processed.noemoticon.csv\")\n",
    "\n",
    "# Download the stopwords if necessary\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Define function to remove non-alphabet characters and convert to lowercase\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Define function to remove stop words\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "# Apply preprocessing steps\n",
    "data['text'] = data['text'].apply(remove_html_tags)\n",
    "data['text'] = data['text'].apply(clean_text)\n",
    "data['text'] = data['text'].str.split()\n",
    "data['text'] = data['text'].apply(remove_stopwords)\n",
    "\n",
    "# Rejoin preprocessed text data into a single string\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Divide Features and Labels\n",
    "X = data[\"text\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# Convert text data to vector representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "#Dimensionality reduction\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "X_reduced = svd.fit_transform(X_vectorized)\n",
    "\n",
    "#Divide training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define the hyperparameter space\n",
    "param_grid = {'n_neighbors': [3, 5, 7]}  # Define the parameter grid for grid search\n",
    "\n",
    "# Perform hyperparameter search and cross-validation\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "\n",
    "# Get program start time\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get program end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate program execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Output the best combination of hyperparameters and program execution time\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"execution time:\", execution_time, \"s\")\n",
    "\n",
    "# predict\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy, precision, recall, confusion matrix\n",
    "accuracy_knn = accuracy_score(y_test, y_pred)\n",
    "precision_knn = precision_score(y_test, y_pred, average='weighted')\n",
    "recall_knn = recall_score(y_test, y_pred, average='weighted')\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# output result\n",
    "print(\"Accuracy: \", accuracy_knn)\n",
    "print(\"Precision: \", precision_knn)\n",
    "print(\"Recall: \", recall_knn)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "####################CNN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import time\n",
    "\n",
    "# read dataset\n",
    "data = pd.read_csv(\"training.300000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None)\n",
    "\n",
    "# Select the text and sentiment columns\n",
    "text = data[5].values\n",
    "sentiment = data[0].values\n",
    "\n",
    "# Map sentiment values to two labels: 0 = negative, 1 = positive\n",
    "sentiment = np.where(sentiment == 0, 0, 1)\n",
    "\n",
    "# Split the text data into training and testing sets\n",
    "train_text, test_text, train_sentiment, test_sentiment = train_test_split(text, sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_length = 100\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length)\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'filters': [64, 128],\n",
    "    'units': [32, 64]\n",
    "}\n",
    "\n",
    "# Create the model to be tuned\n",
    "def create_model(filters, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(units=units, activation='relu'))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=3)\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the grid search\n",
    "grid_result = grid_search.fit(train_data, train_sentiment)\n",
    "\n",
    "# Calculate the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters: \", grid_result.best_params_)\n",
    "\n",
    "# Print the elapsed time\n",
    "print(\"Elapsed Time:\", elapsed_time, \"seconds\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_result.best_estimator_.model\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_predictions = np.argmax(best_model.predict(test_data), axis=-1)\n",
    "accuracy_cnn = accuracy_score(test_sentiment, test_predictions)\n",
    "precision_cnn = precision_score(test_sentiment, test_predictions, average='weighted')\n",
    "recall_cnn = recall_score(test_sentiment, test_predictions, average='weighted')\n",
    "confusion_mat = confusion_matrix(test_sentiment, test_predictions)\n",
    "\n",
    "#output result\n",
    "print(\"Test Accuracy:\", accuracy_cnn)\n",
    "print(\"Precision:\", precision_cnn)\n",
    "print(\"Recall:\", recall_cnn)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "\n",
    "# Create a dictionary to store the model results\n",
    "results = {\n",
    "    'Model': ['Naive Bayes', 'KNN', 'CNN'],\n",
    "    'Accuracy': [accuracy_nb, accuracy_knn, accuracy_cnn],\n",
    "    'Precision': [precision_nb, precision_knn, precision_cnn],\n",
    "    'Recall': [recall_nb, recall_knn, recall_cnn]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the results dictionary\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Set the 'Model' column as the index\n",
    "df_results.set_index('Model', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
